---
sidebar_position: 6
---
import LigaHighlight from '@site/src/components/gsap/highlight'
import BoasVindasChatGPT from '@site/src/components/BoasVindasChatGPTAlucina'
import ReactPlayer from 'react-player'

# Alucina√ß√µes
<LigaHighlight />
:::note

Este conte√∫do est√° presente no guia *Desmistificando a Intelig√™ncia Artificial*.

:::
Ap√≥s o lan√ßamento da vers√£o GPT-4o, um colega prop√¥s um desafio visual a essa nova vers√£o do ChatGPT: analisar a foto de uma <spam class="text-highlight">est√°tua rec√©m-inaugurada</spam>. A resposta peculiar da IA chamou minha aten√ß√£o. Registrei essa experi√™ncia para compartilhar neste cap√≠tulo, pois revela de forma v√≠vida as nuances da intelig√™ncia artificial em a√ß√£o.

<center>
<ReactPlayer url='https://youtu.be/l_N_XcEU71s' width='100%' controls='true' />
</center>
<br />
No v√≠deo, uma est√°tua de bronze de um jogador de futebol est√° posicionada em frente ao Est√°dio Joaquim Am√©rico, casa do Club Athletico Paranaense. No primeiro "desafio" proposto ao ChatGPT, a IA descreve corretamente a est√°tua, mencionando os detalhes visuais e a localiza√ß√£o, <spam class="text-highlight">mas comete um erro</spam> ao identificar o atleta, chamando-o de Alex. Esse erro √© "imperdo√°vel", especialmente para um torcedor atleticano, üôÇ j√° que Alexandro de Souza foi formado nas categorias de base e jogou por muitos anos no rival Coritiba Foot Ball Club. Esse tipo de equ√≠voco, <spam class="text-highlight">conhecido como alucina√ß√£o</spam>, ocorre quando o modelo gera informa√ß√µes incorretas ou inventadas.

Na segunda pergunta, ap√≥s receber orienta√ß√µes mais espec√≠ficas, indicando que o atleta em quest√£o √© o maior artilheiro da hist√≥ria do clube, o ChatGPT <spam class="text-highlight">corrige sua resposta</spam> e identifica corretamente o jogador. Esse exemplo demonstra como fornecer informa√ß√µes contextuais relevantes pode ajudar o modelo a gerar respostas mais precisas.

Alguns dias depois, fiz a mesma pergunta e o ChatGPT respondeu corretamente. Isso ilustra dois pontos importantes: primeiro, as ferramentas de IA est√£o em constante aprimoramento, com ajustes nos algoritmos, atualiza√ß√µes dos dados de treinamento e incorpora√ß√£o de *feedback* dos usu√°rios para corrigir erros e vieses. Segundo, h√° o <spam class="text-highlight">risco de acreditarmos em respostas incorretas</spam>, j√° que os modelos de linguagem geram texto de maneira bastante convincente.

Neste caso de identifica√ß√£o incorreta do jogador que descrevi, o erro n√£o teve consequ√™ncias relevantes, resultando apenas em uma troca de mensagens no WhatsApp entre amigos, comentando a situa√ß√£o. No entanto, os casos da Air Canada e do escrit√≥rio de advocacia Levidow, Levidow & Oberman, que veremos a seguir, mostram que erros em respostas de IA podem ter implica√ß√µes muito mais s√©rias.

## Advogados s√£o multados por usar o ChatGPT
Um caso que ganhou destaque em 2023 envolveu o passageiro Roberto Mata, que processou a companhia a√©rea Avianca alegando ter se ferido quando um carrinho de metal atingiu seu joelho durante um voo para o Aeroporto Internacional Kennedy, em Nova York. 

Quando a Avianca solicitou ao juiz federal de Manhattan o encerramento do caso, os advogados do Sr. Mata se opuseram, apresentando um <spam class="text-highlight">documento de 10 p√°ginas</spam> que citava mais de uma d√∫zia de decis√µes judiciais relevantes.

Para produzir o documento, o advogado respons√°vel pelo caso contra a Avianca utilizou o <spam class="text-highlight">ChatGPT para consultar outros casos semelhantes</spam>, e o chatbot forneceu prontamente uma lista de casos. Para garantir a precis√£o das informa√ß√µes, o advogado pediu ao ChatGPT que confirmasse a veracidade das informa√ß√µes e fornecesse as fontes dos processos. O ChatGPT respondeu novamente, fornecendo essas informa√ß√µes. <spam class="text-highlight">Confiando na IA</spam>, o advogado n√£o verificou as fontes e apresentou as informa√ß√µes no tribunal.

O resultado foi que ningu√©m ‚Äì nem os advogados da companhia a√©rea, nem o pr√≥prio juiz ‚Äì conseguiu encontrar as decis√µes ou cita√ß√µes mencionadas no documento. O juiz considerou a situa√ß√£o 'sem precedentes' e ordenou uma audi√™ncia para avaliar poss√≠veis san√ß√µes contra o advogado. Na audi√™ncia, advogados e escrit√≥rios foram multados em US$ 5.000 por apresentar <spam class="text-highlight">cita√ß√µes falsas em um documento judicial</spam>.

## O risco da alucina√ß√£o
Uma alucina√ß√£o ocorre quando o ChatGPT gera uma resposta que parece plaus√≠vel, mas est√° incorreta ou foi inventada.

O termo 'alucina√ß√£o' √© utilizado porque, assim como uma alucina√ß√£o em seres humanos envolve a percep√ß√£o de algo que n√£o est√° presente na realidade, uma alucina√ß√£o em modelos de linguagem refere-se √† gera√ß√£o de informa√ß√µes sem fundamento real, que podem parecer convincentes e reais.

## Por que a ferramenta alucina?
Ferramentas baseadas em IA, como o ChatGPT, funcionam prevendo as probabilidades de sequ√™ncias de palavras. Cada palavra gerada √© escolhida <spam class="text-highlight">com base nas probabilidades</spam> calculadas a partir do texto atual. Quando o modelo faz uma previs√£o incorreta ou improv√°vel, isso pode resultar em uma 'alucina√ß√£o', ou seja, uma resposta que n√£o √© factualmente correta.

<BoasVindasChatGPT />

O ChatGPT foi treinado em vastos conjuntos de dados de texto, onde aprendeu padr√µes estat√≠sticos e associa√ß√µes. No entanto, quando confrontado com perguntas <spam class="text-highlight">amb√≠guas</spam>, <spam class="text-highlight">incomuns</spam> ou com <spam class="text-highlight">informa√ß√µes insuficientes</spam> nos dados de treinamento, a certeza sobre a pr√≥xima palavra a ser gerada diminui. Isso faz com que a distribui√ß√£o de probabilidade das poss√≠veis pr√≥ximas palavras se torne mais dispersa, ou seja, menos concentrada em uma √∫nica palavra.